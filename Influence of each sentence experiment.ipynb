{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence and Distribution of different sentences\n",
    "\n",
    "In this part of experiment, we look inside the distribution of different sentences with their distance of text. \n",
    "\n",
    "Because if we only consider the one segment in a sentence, we could not get the right `main sentences` usually, we need to find a more general approach to achieve finding the `main sentences` from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from text_summary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utlis.get_word_vector_by_glove import get_consistent\n",
    "from utlis.get_word_vector_by_glove import get_local_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_corelation(corelation, sub_plot=None):\n",
    "    mean = np.mean(corelation)\n",
    "    _1st_percentile = np.percentile(corelation, 25)\n",
    "    _3st_percentile = np.percentile(corelation, 75)\n",
    "    \n",
    "    if sub_plot:\n",
    "        plt.subplot(*sub_plot)\n",
    "    \n",
    "    print('corelation length is: {}'.format(len(corelation)))\n",
    "    plt.plot(range(len(corelation)), corelation, c=(0, 0, 0.2))\n",
    "    plt.fill_between(range(len(corelation)), corelation)\n",
    "    plt.plot([mean] * len(corelation), 'r--')\n",
    "    plt.plot([_1st_percentile] * len(corelation), 'g+')\n",
    "    plt.plot([_3st_percentile] * len(corelation), 'b*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是一篇广告，不适合做摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is the distance of first half of ads and tail half of ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_two_sentence_distance(\"\".join(words[: len(words)//2]), \"\".join(words[len(words)//2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_verbose(corelations):\n",
    "    threshold = 0.03\n",
    "    variance = np.var(corelations)\n",
    "    return (1/variance) * np.log(len(corelations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x, old_min=0, old_max=10):\n",
    "    new_x = (x - old_min) / (old_max - old_min) * 100\n",
    "    return new_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_coherence(sentences):\n",
    "    threshold = 0.5\n",
    "    text = \"\".join(sentences)\n",
    "    half_head = text[:len(text)//2]\n",
    "    half_tail = text[len(text)//2:]\n",
    "    distance = get_two_sentence_distance(half_head, half_tail)\n",
    "    print('distance: {}'.format(distance))\n",
    "    coherence = (1/(distance)) * np.log(len(sentences))\n",
    "    return coherence\n",
    "\n",
    "def is_coherent(text):\n",
    "    coherence = get_coherence(text)\n",
    "    threshold = 0.03\n",
    "    return coherence > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(array):\n",
    "    array = np.array(array)\n",
    "    array -= np.max(array, axis=0)\n",
    "    return np.exp(array) / sum(np.exp(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_text_corelation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-49d6667b3b79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'experiment/test_text.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorelations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_text_corelation' is not defined"
     ]
    }
   ],
   "source": [
    "f = 'experiment/test_text.txt'\n",
    "corelations = get_text_corelation(f)\n",
    "plot_corelation(softmax(corelations[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the distance of Title with each sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corelations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e68476deddd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorelations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corelations' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = [s for s, d in corelations[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title = '早高峰无盖井卡住两辆车 居民盼相关部门解决'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_title_distance(title, sentences):\n",
    "    return get_text_sentences_distances(title, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b62cda2f70ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_title_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "dis = get_title_distance(title, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-60d883f80280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtitle_corelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dis' is not defined"
     ]
    }
   ],
   "source": [
    "title_corelations = softmax([1 - d for _ , d in dis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title_corelations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a6dcc18c56d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_corelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'title_corelations' is not defined"
     ]
    }
   ],
   "source": [
    "plot_corelation(title_corelations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corelations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c53077edbb4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorelations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corelations' is not defined"
     ]
    }
   ],
   "source": [
    "plot_corelation(softmax(corelations[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The corelation with all the text and title is very similar, at least for \"experiment/test_text.txt\" this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The furthure test. \n",
    "\n",
    "In order to get the more general information, we need to test more texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test of bounch texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = ['experiment/ads.txt', 'experiment/test_text.txt', 'experiment/test_text_01.txt', 'experiment/test_text_02.txt', 'experiment/test_text_03.txt', 'experiment/test_text_04.txt', 'experiment/text_06.txt', 'experiment/text_07.txt', \\\n",
    "         'experiment/many_verbose.txt', 'experiment/not_coherent.txt', 'experiment/test_wechat.txt']\n",
    "titles = ['再不去闯，梦想永远只是梦想', '早高峰无盖井卡住两辆车 居民盼相关部门解决', '使馆区武警目击作案过程 围堵拦截小偷', '七国集团峰会联合公报涉及东海、南海问题 中国外交部回应 快看', '端午小长假高速不免费 9个收费站可用支付宝缴费', '轻松一刻：全班15对情侣，这才是最虐狗的班级', \\\n",
    "         '山东省检察院派员出庭于欢故意伤害案二审法庭（出庭意见书全文）', '除了四川人民 最爱打麻将的竟是土耳其', '处女座剧组人设崩塌？穿帮+广告毁了《欢乐颂2', '比亚迪强大中国车王朝擂台赛杭州站开赛', \\\n",
    "         '这个国家每年“六一”前都会给老百姓发钱去度假']\n",
    "assert len(files) == len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_one(f, t, index = 0, f_number=1, fig=None):\n",
    "    fig = fig or plt.figure(figsize=(20, 8))\n",
    "    print(\"{}, {}\".format(f, t))\n",
    "    \n",
    "    corelations = get_text_corelation(f)\n",
    "    sentences = [s for s, d in corelations[2]]\n",
    "    coherence = get_coherence(sentences)\n",
    "    print('coherence == {}'.format(coherence))\n",
    "\n",
    "    fig.add_subplot(f_number, 3, index * 3 + 1)\n",
    "    plot_corelation(softmax(corelations[1]))\n",
    "    title_distance = get_title_distance(t, sentences)\n",
    "    title_corelation = [1 - d for _, d in title_distance]\n",
    "    fig.add_subplot(f_number, 3, index * 3 + 2)\n",
    "    plot_corelation(softmax(title_corelation))\n",
    "    \n",
    "    fig.add_subplot(f_number, 3, index * 3 + 3)\n",
    "    plot_corelation(softmax(corelations[1]))\n",
    "    plot_corelation(softmax(title_corelation))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_mutiply_f_t(f_t):\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    index = 0\n",
    "    for f, t in f_t:\n",
    "        plot_one(f, t, index, f_number=len(f_t), fig=fig)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_t = list(zip(files, titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment/ads.txt, 再不去闯，梦想永远只是梦想\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_text_corelation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4bf8c9102f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_mutiply_f_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d9a8ff0e96a0>\u001b[0m in \u001b[0;36mplot_mutiply_f_t\u001b[0;34m(f_t)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mplot_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-fdf968e2013d>\u001b[0m in \u001b[0;36mplot_one\u001b[0;34m(f, t, index, f_number, fig)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}, {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcorelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorelations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcoherence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_text_corelation' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1160b9a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mutiply_f_t(f_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_one(*f_t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Accumulate Corelation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accumulate(x):\n",
    "    x = np.array(x)\n",
    "    acc = [np.sum(x[:(index+1)]) for index in range(len(x))]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulate([1, 2, 3]) == [1, 3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_complex_corelation(title_corelation, content_corelation):\n",
    "    p = 0.5\n",
    "    return softmax(p * title_corelation + (1 - p) * content_corelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_one_file_complex_corelation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1c115bddda66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'experiment/text_07.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcomplex_corelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_one_file_complex_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'most trivial '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_one_file_complex_corelation' is not defined"
     ]
    }
   ],
   "source": [
    "file_index = files.index('experiment/text_07.txt')\n",
    "complex_corelation = get_one_file_complex_corelation(files[file_index], titles[file_index])\n",
    "sentences = get_text_sentence(files[file_index])\n",
    "\n",
    "print('most trivial ')\n",
    "for index, s in enumerate(sentences):\n",
    "    if complex_corelation[index] < np.percentile(complex_corelation, 25):\n",
    "        print(s)\n",
    "        \n",
    "print('most important')\n",
    "for index, s in enumerate(sentences):\n",
    "    if complex_corelation[index] > np.percentile(complex_corelation, 75):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_outliner(x, array):\n",
    "    _1st_percentile = np.percentile(array, 25)\n",
    "    _3th_percentile = np.percentile(array, 75)\n",
    "    threshold = 1.5\n",
    "    if (_1st_percentile / x) > threshold or (x / _3th_percentile) > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_outliner(Xs):\n",
    "    Xs = np.array(Xs)\n",
    "    Xs = list(filter(lambda x: not is_outliner(x, Xs), Xs))\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 12, 4, 14, -1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_outliner([1, 2, 1, 43, 12, 4, 14, -1, 999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_2_loss(y_hats, ys):\n",
    "    def f(x):\n",
    "        max_length = 1000\n",
    "        if x > max_length: return max_length\n",
    "        else: return x\n",
    "    return np.sum(np.square(y_hats - ys))  * 1 / f(len(y_hats))\n",
    "\n",
    "def have_main_point(complex_corelation, plot=True):\n",
    "    if len(complex_corelation) < 10: \n",
    "        return False, -1\n",
    "    else:\n",
    "        print('before clean length is: {}'.format(len(complex_corelation)))\n",
    "        complex_corelation = clean_outliner(complex_corelation)\n",
    "        print('end clean legnth is: {}'.format(len(complex_corelation)))\n",
    "        k = (1.0 - 0) / (len(complex_corelation) - 0)\n",
    "        ys = k * np.arange(0, len(complex_corelation))\n",
    "        acc = accumulate(complex_corelation)\n",
    "        if plot: plot_corelation(acc)    \n",
    "        l2_loss = l_2_loss(acc, ys)\n",
    "        threshold = 5.0e-4\n",
    "        logging.info(l2_loss)\n",
    "        print(\"l2-loss: {}\".format(l2_loss))\n",
    "        if l2_loss < threshold:\n",
    "            return False, l2_loss\n",
    "        else:\n",
    "            return True, l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_to_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_if_one_file_fit_summary(text, title):\n",
    "    mini_length = 200\n",
    "    if not os.path.isfile(text) and len(text) < mini_length: \n",
    "        return False, -1\n",
    "    else:\n",
    "        complex_corelation = get_one_file_complex_corelation(text, title)\n",
    "        return fit_to_summary(complex_corelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distinct(array):\n",
    "    reversed_array = array[::-1]\n",
    "    for index, e in enumerate(reversed_array[:-1]):\n",
    "        if e in reversed_array[index+1:]: array[-(1+index)] = None\n",
    "    array  = [e for e in array if e is not None]\n",
    "    return array\n",
    "\n",
    "def get_main_sentene_with_theme_corelation(sentences, corelations):\n",
    "    print(sorted(zip(sentences, corelations), key=lambda x: x[1], reverse=True))\n",
    "    important = clean_outliner(corelations)\n",
    "    avg_length = np.mean([len(sentence) for sentence in sentences])\n",
    "    max_sentences_num = int(200/avg_length)\n",
    "    important = distinct(important)\n",
    "    most_important = np.sort(important)[::-1][:max_sentences_num]\n",
    "    most_important_sentences = distinct([s for i, s in enumerate(sentences) if corelations[i] in most_important])\n",
    "    print(most_important)\n",
    "    return most_important_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def in_same_sentence(subsentence1, subsentence2, text):\n",
    "    begin_index = text.index(subsentence1)\n",
    "    if find_complete_sentence(subsentence1, text[begin_index:]) == find_complete_sentence(subsentence2, text[begin_index:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def get_complete_sentences_with_corelations(sentences, single_subsentence_corelations, text):\n",
    "    complete_sentences = []\n",
    "    complete_sentences_corelations = []\n",
    "\n",
    "    corelations = [None if is_outliner(x, single_subsentence_corelations) else x for x in single_subsentence_corelations]\n",
    "    single_complete_corelations = [corelations[0]]\n",
    "    single_complete_sentence = [sentences[0]]\n",
    "    \n",
    "    for index in range(1, len(sentences)):\n",
    "        sub = sentences[index]\n",
    "        last_word = single_complete_sentence[-1]\n",
    "        if in_same_sentence(last_word, sub, text):\n",
    "            single_complete_sentence.append(sub)\n",
    "            single_complete_corelations.append(corelations[index])\n",
    "        else:\n",
    "            complete_sentences.append(single_complete_sentence)\n",
    "            complete_sentences_corelations.append(single_complete_corelations)\n",
    "            \n",
    "            single_complete_sentence = [sub]\n",
    "            single_complete_corelations = [corelations[index]]\n",
    "            \n",
    "        if index == len(sentences) - 1:\n",
    "            complete_sentences.append(single_complete_sentence)\n",
    "            complete_sentences_corelations.append(single_complete_corelations)\n",
    "    \n",
    "    return zip(complete_sentences, complete_sentences_corelations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_summary_with_fit_summary_test(text, title):\n",
    "    ## get the main sentences of one text\n",
    "    ## if no main sentene or not fit to get summary, return None else return sentences\n",
    "    mini_length = 250\n",
    "    complex_corelation = get_one_file_complex_corelation(text, title)\n",
    "    if not os.path.isfile(text) and len(text) < mini_length: return None\n",
    "    elif have_main_point(complex_corelation, plot=False)[0]:\n",
    "        sentences = get_text_sentence(text)\n",
    "        main_sentences = get_main_sentene_with_theme_corelation(sentences, complex_corelation)\n",
    "        print('main sentences: {}'.format(main_sentences))\n",
    "        text_content = get_text_content(text)\n",
    "        summary = change_sentences_to_complete_sentences(main_sentences, text_content)\n",
    "        summary = \"\".join(summary)\n",
    "        return summary\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_sentences_to_complete_sentences(sub_sentences, text):\n",
    "    entire_sentences = [find_complete_sentence(sub_str, text) for sub_str in sub_sentences]\n",
    "    entire_sentences = distinct(entire_sentences)\n",
    "    return entire_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anaylsis No-Linear Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text_corelation(text):\n",
    "    text_sentences = get_text_sentence(text)\n",
    "    distance_map = get_all_sentences_distance(text_sentences)\n",
    "    distance_sentence_pair = [(string, distance_map[string]) for string in text_sentences]\n",
    "    corelation = [1 - d for _, d in distance_sentence_pair]\n",
    "    segments_with_index = [index_word for index_word in enumerate(text_sentences)]\n",
    "    return segments_with_index, corelation, distance_sentence_pair\n",
    "\n",
    "\n",
    "def get_one_file_complex_corelation(text, title):\n",
    "#    print(\"{} {}\".format(text[:50], title))\n",
    "        \n",
    "    corelations = get_text_corelation(text)\n",
    "    sentences = [s for s, d in corelations[2]]\n",
    "    title_distance = get_title_distance(title, sentences)\n",
    "    title_corelation = softmax([1 - d for _, d in title_distance])\n",
    "    content_corelation = softmax(corelations[1])\n",
    "    complex_corelation = (title_corelation, content_corelation)\n",
    "    #plot_corelation(complex_corelation[0])\n",
    "    return complex_corelation[0]\n",
    "\n",
    "def get_summary_with_nolinear(text, title):\n",
    "    complex_corelation = get_one_file_complex_corelation(text, title)\n",
    "    sentences = get_text_sentence(text)\n",
    "    complete_no_linear = get_complete_sentences_with_corelations(\n",
    "        sentences, complex_corelation, get_text_content(text, escape_english=False))\n",
    "\n",
    "    def f(array, total_words_length):\n",
    "        array = list(filter(lambda x: x is not None, array))\n",
    "        result = np.mean(array) * (1.05 ** (total_words_length-1))\n",
    "        return result if not np.isnan(result) else -1\n",
    "\n",
    "    def get_merged_corelation(single_nolinear_corelations, sentences):\n",
    "        total_words_length = len(\"\".join(sentences))\n",
    "        merged_corelation = f(single_nolinear_corelations, total_words_length)\n",
    "        return merged_corelation\n",
    "\n",
    "    def get_sentence_and_merged_corelation(subsentences, corelation):\n",
    "        return (\" \".join(subsentences), corelation)\n",
    "\n",
    "    completed_sentences_with_corelations = []\n",
    "    \n",
    "    for s, c in complete_no_linear:\n",
    "        merged_corelation = get_merged_corelation(c, s)\n",
    "        single_completed_sentence_with_corelation = get_sentence_and_merged_corelation(s, merged_corelation)\n",
    "        completed_sentences_with_corelations.append(single_completed_sentence_with_corelation)\n",
    "\n",
    "    _25_percentile = np.percentile([c for s, c in completed_sentences_with_corelations], 25)\n",
    "    _60_percentile = np.percentile([c for s, c in completed_sentences_with_corelations], 60)\n",
    "\n",
    "    sentences = list(map(lambda x: x[0], completed_sentences_with_corelations))\n",
    "    corelations = list(map(lambda x: x[1], completed_sentences_with_corelations))\n",
    "    corelations = [x if not np.isnan(x) else -1 for x in corelations]\n",
    "\n",
    "    total_sentence = []\n",
    "    total_length = 0\n",
    "    min_single_length = 3\n",
    "    for string, corelations in zip(sentences, corelations):\n",
    "        if corelations > _60_percentile:\n",
    "            if len(string) >= min_single_length:\n",
    "                total_length += len(string)\n",
    "                total_sentence.append(string)\n",
    "    \n",
    "    return \"。\".join(total_sentence)\n",
    "\n",
    "def get_suitable_length_summary(text, title, fit_length):\n",
    "    summary = get_summary_with_nolinear(text, title)\n",
    "    if len(summary) > fit_length:\n",
    "        return get_summary_with_nolinear(summary, title)\n",
    "    else:\n",
    "        return summary\n",
    "    \n",
    "def readable_summary(text, title):\n",
    "    fit = test_if_one_file_fit_summary(text, title)\n",
    "    if fit:\n",
    "        fit_length = get_fit_length(len(get_text_content(text)))\n",
    "        return title + \": \" + get_suitable_length_summary(text, title, fit_length)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fit_length(original_length):\n",
    "    length_map = {\n",
    "        lambda x: x > 1000: 250,\n",
    "        lambda x: 500 < x < 1000: 200,\n",
    "        lambda x: 300 < x < 500: 150, \n",
    "        lambda x: x < 300: 100\n",
    "    }\n",
    "    for cond in length_map:\n",
    "        if cond(original_length):\n",
    "            return length_map[cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_file_path = 'experiment/error_analysis.txt'\n",
    "title = '“天面”整治让平遥古城更古色古香'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = get_text_content(target_file_path, escape_english=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.index('22日')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'晚'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'中新网5月24日电 据外媒24日报道，警方称，3名男子疑与22日晚的曼彻斯特体育场爆炸袭击有关联，在曼彻斯特南部地区被捕。'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_complete_sentence('22日', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_length = len(get_text_content(target_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fit_length(original_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment/error_analysis.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kouminquan/anaconda/envs/env-3/lib/python3.4/site-packages/ipykernel/__main__.py:5: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Users/kouminquan/anaconda/envs/env-3/lib/python3.4/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/kouminquan/anaconda/envs/env-3/lib/python3.4/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花旗：部分中国国企几乎肯定会被穆迪调降评级: 整治与古城风貌不协调的彩钢瓦天面、红色墙面、防水屋面 工程喷涂电力箱体设施 清理屋顶乱堆乱放杂物 连日来 平遥县疏堵结合、积极推进“天面”整治工程 让古城更显古色古香。在此基础上 组织规划、城管、文物等部门50余人 对古城内不协调建筑天面的位置、业主、面积、材料等情况 进行了全面的大排查、大摸底 并根据蓝色彩钢瓦天面、红色墙面、SBS防水屋面属性 逐户建立台账 销号管理。在工程实施过程中 平遥县将古城分成东南西北4大片区 组织4支专业施工队伍 按照先城墙目击范围、后中心区域 先主干街道、后小街小巷的顺序 由外及里 逐院逐户 分层推进 全方位、立体式推进不协调建筑天面整治工作 根据不同情况、不同材质 进行分类整治。对整院搭建的彩钢瓦、二层及二层以上彩钢瓦屋顶、临街临巷的遮雨遮阳彩钢瓦等严重影响古城风貌的 坚决予以了拆除 对院内搭设的遮雨遮阳彩钢瓦、建筑屋顶铺设的防水屋面及红色墙面及电力箱体设施 进行了工程喷涂或更换砖灰色彩钢顶\n"
     ]
    }
   ],
   "source": [
    "summary = readable_summary(target_file_path, title)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = 'data_preprocess/updated_news/sqlResult_1262716_0524.csv'\n",
    "r = csv.reader(open(csv_file))\n",
    "lines = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = 'experiment/test_if_fit_summary_with_artifical.csv'\n",
    "r = csv.reader(open(csv_file))\n",
    "lines = [l for l in r]\n",
    "for line in lines[1:]:\n",
    "    title = line[1]\n",
    "    content = line[2]\n",
    "    fit = test_if_one_file_fit_summary(content, title)\n",
    "    line[4] = fit[1]\n",
    "    \n",
    "writer = csv.writer(open(csv_file, 'w', newline=''))\n",
    "writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit_to_summary(complex_corelation)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'experiment/fit_summary_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_from_my_file(file, mark):\n",
    "    lines = [line for line in open(file).readlines() if line.startswith(mark)]\n",
    "    ids = [int(line.strip()[len(mark):]) for line in lines]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDs = get_num_from_my_file(file_path, 'ID: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fits = get_num_from_my_file(file_path, 'fit:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(ids) == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_to_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-28ec7a9b2762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadable_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-194-37720475cb93>\u001b[0m in \u001b[0;36mreadable_summary\u001b[0;34m(text, title)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadable_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_if_one_file_fit_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mfit_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fit_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_text_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-bf06429a78b3>\u001b[0m in \u001b[0;36mtest_if_one_file_fit_summary\u001b[0;34m(text, title)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcomplex_corelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_one_file_complex_corelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfit_to_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_corelation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fit_to_summary' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "test_file = 'test_summary_0613.csv'\n",
    "csv_file = 'data_preprocess/updated_news/sqlResult_1262716_0524.csv'\n",
    "contents = pd.read_csv(csv_file)\n",
    "contents = contents.iterrows()\n",
    "differents = []\n",
    "test = []\n",
    "length = 2\n",
    "with open(test_file, 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter.writerow(['id', 'title', 'content', 'description', 'score'])\n",
    "    for C in contents:\n",
    "        if length <= 0: break\n",
    "        ID, content, title = C[1][0], C[1][4], C[1][2]\n",
    "        if random.random() < 0.8: continue\n",
    "        summary = readable_summary(content, title)\n",
    "\n",
    "        if summary:\n",
    "            spamwriter.writerow([ID, title, content, summary, ''])\n",
    "            length -= 1\n",
    "            print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = '黄小蕾控诉迪士尼工作人员故意刁难 工作态度冷漠'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_if_one_file_fit_summary(content, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "differents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter[0] / counter[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(fits, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(fits, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold == 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold == 1.5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_nagative = 7\n",
    "false_positive = 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
